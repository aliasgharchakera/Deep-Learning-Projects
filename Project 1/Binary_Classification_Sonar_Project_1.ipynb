{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSuJa_Aa1lL9"
      },
      "source": [
        "# Project 1: Binary Classification Sonar for the Navy\n",
        "In this project you will discover how to effectively use the Keras library in your machine learning project by working through a binary classification project step-by-step.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hUX_qOm1lMB"
      },
      "source": [
        "<a name=\"1\"></a>\n",
        "## 1 - Packages \n",
        "\n",
        "We will start off by importing all of the classes and functions we will need:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikeras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEouUcYerT64",
        "outputId": "fde16b7b-2283-4034-aa66-71b312b95e56"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting scikeras\n",
            "  Downloading scikeras-0.10.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: packaging>=0.21 in /usr/local/lib/python3.10/dist-packages (from scikeras) (23.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikeras) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (3.1.0)\n",
            "Installing collected packages: scikeras\n",
            "Successfully installed scikeras-0.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cCAgAxGY1lMC"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "import pandas\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "# from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can load the dataset using pandas and split the columns into 60 input variables (X) and 1 output variable (Y). We use pandas to load the data because it easily handles strings (the output variable), whereas attempting to load the data directly using NumPy would be more difficult."
      ],
      "metadata": {
        "id": "DUedIe48yuQK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nkdSl9WE1lMG"
      },
      "outputs": [],
      "source": [
        "# load dataset\n",
        "dataframe = pandas.read_csv(\"sonar.csv\", header=None)\n",
        "dataset = dataframe.values\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:60].astype(float)\n",
        "Y = dataset[:,60]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output variable is string values. We must convert them into integer values 0 and 1.\n",
        "\n",
        "We can do this using the LabelEncoder class from scikit-learn. This class will model the encoding required using the entire dataset via the fit() function, then apply the encoding to create a new output variable using the transform() function.\n"
      ],
      "metadata": {
        "id": "1mNpHXe4y0S0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qI9tIfqH1lMG"
      },
      "outputs": [],
      "source": [
        "# encode class values as integers\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(Y)\n",
        "encoded_Y = encoder.transform(Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Letâ€™s start off by defining the function that creates our baseline model. Our model will have a single fully connected hidden layer with the same number of neurons as input variables. This is a good default starting point when creating neural networks.\n",
        "\n",
        "The weights are initialized using a small Gaussian random number. The Rectifier activation function is used. The output layer contains a single neuron in order to make predictions. It uses the sigmoid activation function in order to produce a probability output in the range of 0 to 1 that can easily and automatically be converted to crisp class values.\n",
        "\n",
        "Finally, we are using the logarithmic loss function (binary_crossentropy) during training, the preferred loss function for binary classification problems. The model also uses the efficient Adam optimization algorithm for gradient descent and accuracy metrics will be collected when the model is trained.\n"
      ],
      "metadata": {
        "id": "mTyHMjXyy7dA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "194VKcBJ1lMI"
      },
      "outputs": [],
      "source": [
        "# baseline model\n",
        "def create_baseline():\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(60, input_dim=60, kernel_initializer='normal', activation='relu'))\n",
        "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
        "    # Compile model\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it is time to evaluate this model using stratified cross validation in the scikit-learn framework.\n",
        "\n",
        "We pass the number of training epochs to the KerasClassifier, again using reasonable default values. Verbose output is also turned off given that the model will be created 10 times for the 10-fold cross validation being performed.\n"
      ],
      "metadata": {
        "id": "-200knAey_FA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMAFTOb91lMJ",
        "outputId": "356fc032-f47f-4d77-f507-c74ea157f17d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results: 80.26% (8.22%)\n"
          ]
        }
      ],
      "source": [
        "# evaluate model with standardized dataset\n",
        "estimator = KerasClassifier(model=create_baseline, epochs=100, batch_size=5, verbose=0)\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "results = cross_val_score(estimator, X, encoded_Y, cv=kfold)\n",
        "print(\"Results: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "An effective data preparation scheme for tabular data when building neural network models is standardization. This is where the data is rescaled such that the mean value for each attribute is 0 and the standard deviation is 1. This preserves Gaussian and Gaussian-like distributions whilst normalizing the central tendencies for each attribute."
      ],
      "metadata": {
        "id": "iZTQ3gKozHDR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ZChGbBIR1lMK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aff319e2-baa5-4546-d465-fcaee34f9f08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standardized: 84.10% (6.66%)\n"
          ]
        }
      ],
      "source": [
        "# evaluate baseline model with standardized dataset\n",
        "numpy.random.seed(seed)\n",
        "estimators = []\n",
        "estimators.append(('standardize', StandardScaler()))\n",
        "estimators.append(('mlp', KerasClassifier(model=create_baseline, epochs=100, batch_size=5, verbose=0)))\n",
        "pipeline = Pipeline(estimators)\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
        "print(\"Standardized: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We suspect that there is a lot of redundancy in the input variables for this project.\n",
        "\n",
        "The data describes the same signal from different angles. Perhaps some of those angles are more relevant than others. We can force a type of feature extraction by the network by restricting the representational space in the first hidden layer.\n",
        "\n",
        "In this experiment, we take our baseline model with 60 neurons in the hidden layer and reduce it by half to 30. This will put pressure on the network during training to pick out the most important structure in the input data to model.\n",
        "\n",
        "We will also standardize the data as in the previous experiment with data preparation and try to take advantage of the small lift in performance.\n"
      ],
      "metadata": {
        "id": "yihjZxk0zLe1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# smaller model\n",
        "def create_smaller():\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(60, input_dim=60, kernel_initializer=\"normal\", activation=\"relu\"))\n",
        "    model.add(Dense(1, kernel_initializer=\"normal\", activation=\"sigmoid\"))\n",
        "    \n",
        "    # Compile model\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "estimators = []\n",
        "estimators.append(('standardize', StandardScaler()))\n",
        "estimators.append(('mlp', KerasClassifier(model=create_smaller, epochs=100, batch_size=5, verbose=0)))\n",
        "pipeline = Pipeline(estimators)\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
        "print(\"Smaller: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snHwnXUWsO4d",
        "outputId": "4fdb0019-6b57-4402-dc34-797af7037fc4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Smaller: 83.60% (8.07%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A neural network topology with more layers offers more opportunity for the network to extract key features and recombine them in useful nonlinear ways.\n",
        "\n",
        "We can evaluate whether adding more layers to the network improves the performance easily by making another small tweak to the function used to create our model. Here, we add one new layer (one line) to the network that introduces another hidden layer with 30 neurons after the first hidden layer.\n"
      ],
      "metadata": {
        "id": "rtt0R0NwzPqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# larger model\n",
        "def create_larger():\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(60, input_dim=60, kernel_initializer=\"normal\", activation=\"relu\"))\n",
        "    model.add(Dense(30, kernel_initializer=\"normal\", activation=\"relu\"))\n",
        "    model.add(Dense(1, kernel_initializer=\"normal\", activation=\"sigmoid\"))\n",
        "    # Compile model\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\t\n",
        "    return model\n",
        "\n",
        "estimators = []\n",
        "estimators.append(('standardize', StandardScaler()))\n",
        "estimators.append(('mlp', KerasClassifier(model=create_larger, epochs=100, batch_size=5, verbose=0)))\n",
        "pipeline = Pipeline(estimators)\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
        "print(\"Larger: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "531a8b2e-f01a-41c8-bfcb-78ac79571cc3",
        "id": "wMqCfbh9vQw4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Larger: 83.64% (6.56%)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}