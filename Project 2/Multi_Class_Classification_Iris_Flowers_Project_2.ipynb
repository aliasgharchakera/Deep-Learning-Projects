{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kN2ZAJ04lG4t"
      },
      "source": [
        "# Project 2: Multi-Class Classification Iris Flowers for Mothers who love Gardening and Flowers: Identifying Flower Types\n",
        "\n",
        "In this project, you will discover how you can use Keras to develop and evaluate neural network models for multi-class classification projects.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqug5hC_lG4x"
      },
      "source": [
        "<a name=\"1\"></a>\n",
        "## 1 - Packages \n",
        "\n",
        "We will start off by importing all of the classes and functions we will need:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikeras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asB5tHbMndFm",
        "outputId": "41a64103-eaff-473d-a3c5-2cdc69b4336b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting scikeras\n",
            "  Downloading scikeras-0.10.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: packaging>=0.21 in /usr/local/lib/python3.10/dist-packages (from scikeras) (23.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikeras) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (3.1.0)\n",
            "Installing collected packages: scikeras\n",
            "Successfully installed scikeras-0.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "1FnhaO5blG4y"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "import pandas\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "# from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from scikeras.wrappers import KerasClassifier, KerasRegressor\n",
        "from keras.utils import np_utils\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset can be loaded directly. Because the output variable contains strings, it is easiest to load the data using pandas. We can then split the attributes (columns) into input variables (X) and output variables (Y)."
      ],
      "metadata": {
        "id": "1xBRf2uU2pcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "dataframe = pandas.read_csv(\"iris.csv\", header=None)\n",
        "dataset = dataframe.values\n",
        "X = dataset[:,0:4].astype(float)\n",
        "Y = dataset[:,4]\n"
      ],
      "metadata": {
        "id": "H9mycDZKlTw7"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When modeling multi-class classification problems using neural networks, it is good practice to reshape the output attribute from a vector that contains values for each class value to be a matrix with a boolean for each class value and whether or not a given instance has that class value or not."
      ],
      "metadata": {
        "id": "xBEPKz9x2mgu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# encode class values as integers\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(Y)\n",
        "encoded_Y = encoder.transform(Y)\n",
        "# convert integers to dummy variables (i.e. one hot encoded)\n",
        "dummy_y = np_utils.to_categorical(encoded_Y)\n"
      ],
      "metadata": {
        "id": "P3ilNJU1lYiu"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is a function that will create a baseline neural network for the iris classification problem. It creates a simple fully connected network with one hidden layer that contains 8 neurons.\n",
        "\n",
        "The hidden layer uses a rectifier activation function, which is a good practice. Because we used a one-hot encoding for our iris dataset, the output layer must create 3 output values, one for each class. The output value with the largest value will be taken as the class predicted by the model.\n"
      ],
      "metadata": {
        "id": "qyKHK-bB3Se3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define baseline model\n",
        "def baseline_model():\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(8, input_dim=4, activation='relu'))\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "    # compile model\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    \n",
        "    return model\n"
      ],
      "metadata": {
        "id": "GReHxAY2lbXV"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now create our KerasClassifier for use in scikit-learn.\n",
        "\n",
        "We can also pass arguments in the construction of the KerasClassifier class that will be passed on to the fit() function internally used to train the neural network. Here, we pass the number of epochs as 200 and batch size as 5 to use when training the model. Debugging is also turned off when training by setting verbose to 0.\n"
      ],
      "metadata": {
        "id": "bSvuMC4a3XOg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "estimator = KerasClassifier(model=baseline_model, epochs=200, batch_size=5, verbose=0)\n"
      ],
      "metadata": {
        "id": "8kfDoCd6mcZU"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now evaluate the neural network model on our training data.\n",
        "\n",
        "The scikit-learn has excellent capability to evaluate models using a suite of techniques. The gold standard for evaluating machine learning models is k-fold cross validation.\n",
        "\n",
        "First we can define the model evaluation procedure. Here, we set the number of folds to be 10 (an excellent default) and to shuffle the data before partitioning it.\n"
      ],
      "metadata": {
        "id": "mldVNOxj3YOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "results = cross_val_score(estimator, X, dummy_y, cv=kfold, error_score='raise')\n",
        "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PUs7GzYm-8a",
        "outputId": "f4321160-b52e-4d6a-c635-be838f813ca3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f06fffcc280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f070da6d2d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline: 98.67% (2.67%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using a smaller model"
      ],
      "metadata": {
        "id": "gOXsaI9B3cFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# smaller model\n",
        "def create_smaller():\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(4, input_dim=4, activation=\"relu\"))\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "    # compile model\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "estimator = KerasClassifier(model=create_smaller, epochs=200, batch_size=5, verbose=0)\n",
        "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "results = cross_val_score(estimator, X, dummy_y, cv=kfold, error_score='raise')\n",
        "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tt6ClPXpzlA9",
        "outputId": "cead7a2f-43fe-4486-f335-bee7bb578c0b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline: 97.33% (4.42%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A neural network topology with more layers offers more opportunity for the network to extract key features and recombine them in useful nonlinear ways.\n",
        "\n",
        "We can evaluate whether adding more layers to the network improves the performance easily by making another small tweak to the function used to create our model. \n",
        "\n",
        "The idea here is that the network is given the opportunity to model all input variables before being bottlenecked and forced to halve the representational capacity, much like we did in the experiment above with the smaller network.\n",
        "\n",
        "Instead of squeezing the representation of the inputs themselves, we add hidden layers to aid in the process.\n"
      ],
      "metadata": {
        "id": "4OsIXlY_3ne1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# larger model\n",
        "def create_larger():\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(8, input_dim=4, activation=\"relu\"))\n",
        "    model.add(Dense(4, activation=\"relu\"))\n",
        "    model.add(Dense(3, activation=\"softmax\"))\n",
        "    # Compile model\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\t\n",
        "    return model\n",
        "\n",
        "estimator = KerasClassifier(model=create_larger, epochs=200, batch_size=5, verbose=0)\n",
        "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "results = cross_val_score(estimator, X, dummy_y, cv=kfold, error_score='raise')\n",
        "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3s2-wgM0HDi",
        "outputId": "f6560ea9-01e2-4857-a8cf-92d7f41d880a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline: 98.00% (4.27%)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}